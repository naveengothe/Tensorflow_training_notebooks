{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fo8cRWOBU2Q",
        "outputId": "a5b471be-8fab-4239-8c5d-bdad791ea312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1000 cost = 0.477011\n",
            "Epoch: 2000 cost = 0.095235\n",
            "Epoch: 3000 cost = 0.038383\n",
            "Epoch: 4000 cost = 0.019316\n",
            "Epoch: 5000 cost = 0.010693\n",
            "[['i', 'like'], ['i', 'love'], ['i', 'hate']] -> ['dog', 'coffee', 'milk']\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "sentences = [\"i like dog\", \"i love coffee\", \"i hate milk\"]\n",
        "\n",
        "word_list = \" \".join(sentences).split()\n",
        "word_list = list(set(word_list))\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "number_dict = {i: w for i, w in enumerate(word_list)}\n",
        "n_class = len(word_dict)  # number of Vocabulary\n",
        "\n",
        "# NNLM Parameters\n",
        "n_step = 2  # number of steps ['i like', 'i love', 'i hate']\n",
        "n_hidden = 2  # number of hidden units\n",
        "\n",
        "def make_batch(sentences):\n",
        "    input_batch = []\n",
        "    target_batch = []\n",
        "\n",
        "    for sen in sentences:\n",
        "        word = sen.split()\n",
        "        input = [word_dict[n] for n in word[:-1]]\n",
        "        target = word_dict[word[-1]]\n",
        "\n",
        "        input_batch.append(np.eye(n_class)[input])\n",
        "        target_batch.append(np.eye(n_class)[target])\n",
        "\n",
        "    return input_batch, target_batch\n",
        "\n",
        "# Custom Model\n",
        "class NNLM(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(NNLM, self).__init__()\n",
        "        self.H = tf.Variable(tf.random.normal([n_step * n_class, n_hidden]))\n",
        "        self.d = tf.Variable(tf.random.normal([n_hidden]))\n",
        "        self.U = tf.Variable(tf.random.normal([n_hidden, n_class]))\n",
        "        self.b = tf.Variable(tf.random.normal([n_class]))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input = tf.reshape(inputs, shape=[-1, n_step * n_class])\n",
        "        tanh = tf.nn.tanh(tf.matmul(input, self.H) + self.d)\n",
        "        logits = tf.matmul(tanh, self.U) + self.b\n",
        "        return logits\n",
        "\n",
        "model = NNLM()\n",
        "\n",
        "# Loss function\n",
        "def loss_fn(model, inputs, targets):\n",
        "    logits = model(inputs)\n",
        "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=targets))\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.optimizers.Adam(0.001)\n",
        "\n",
        "# Training\n",
        "input_batch, target_batch = make_batch(sentences)\n",
        "\n",
        "for epoch in range(5000):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = loss_fn(model, input_batch, target_batch)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "# Predict\n",
        "logits = model(input_batch)\n",
        "predict = tf.argmax(logits, 1).numpy()\n",
        "\n",
        "# Test\n",
        "input = [sen.split()[:2] for sen in sentences]\n",
        "print([sen.split()[:2] for sen in sentences], '->', [number_dict[n] for n in predict])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Text-CNN Parameters\n",
        "embedding_dim = 50\n",
        "max_sequence_length = 10  # Maximum sequence length (increased from 5 to 10)\n",
        "num_classes = 2\n",
        "filter_sizes = [2, 3, 2]  # Adjusted filter sizes\n",
        "num_filters = 128\n",
        "dropout_rate = 0.7\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "# Sample sentences and labels\n",
        "sentences = [\"i love\",\"i loves\",\"i love you\", \"he loves me\", \"she likes baseball\", \"i hate you\", \"sorry for that\", \"this is awful\", \"he is wrost\" , \"this is bad\"]\n",
        "labels = [1,1,1, 1, 1, 0, 0, 0,0,0]  # 1 is good, 0 is not good.\n",
        "\n",
        "# Tokenize and pad the sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "labels = tf.keras.utils.to_categorical(labels, num_classes=num_classes)\n",
        "\n",
        "# Define the Text CNN model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
        "                              output_dim=embedding_dim,\n",
        "                              input_length=max_sequence_length),\n",
        "    *[\n",
        "        tf.keras.layers.Conv1D(filters=num_filters, kernel_size=filter_size, activation='relu')\n",
        "        for filter_size in filter_sizes\n",
        "    ],\n",
        "    tf.keras.layers.GlobalMaxPooling1D(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(dropout_rate),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(sequences, labels, batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "# Test the model with new text\n",
        "test_text = [\"i love coding\", \"he loves me\", \"he is wrost\",\"she likes baseball\",\"this is a bad movie\"]\n",
        "test_sequences = tokenizer.texts_to_sequences(test_text)\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "predictions = model.predict(test_sequences)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "for i, text in enumerate(test_text):\n",
        "    if predicted_labels[i] == 1:\n",
        "        print(f\"'{text}' is good.\")\n",
        "    else:\n",
        "        print(f\"'{text}' is not good.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMBPtZMnGvNJ",
        "outputId": "955141cc-269b-45e0-f077-1f61390c5fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6980 - accuracy: 0.3000\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6936 - accuracy: 0.5000\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6917 - accuracy: 0.6000\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6894 - accuracy: 0.6000\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6766 - accuracy: 0.9000\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6798 - accuracy: 0.7000\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6768 - accuracy: 0.8000\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6725 - accuracy: 0.9000\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6554 - accuracy: 0.9000\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6685 - accuracy: 0.8000\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "'i love coding' is good.\n",
            "'he loves me' is good.\n",
            "'he is wrost' is not good.\n",
            "'she likes baseball' is good.\n",
            "'this is a bad movie' is not good.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load a smaller subset of the IMDB dataset\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=1000)  # Use only the top 1000 most frequent words\n",
        "\n",
        "# Preprocess the data\n",
        "max_sequence_length = 200\n",
        "train_data = pad_sequences(train_data, maxlen=max_sequence_length)\n",
        "test_data = pad_sequences(test_data, maxlen=max_sequence_length)\n",
        "\n",
        "# Build a log-linear text classification model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=1000, output_dim=16, input_length=max_sequence_length),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_data, train_labels,\n",
        "                    epochs=10,\n",
        "                    batch_size=512,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "results = model.evaluate(test_data, test_labels)\n",
        "print(\"Test loss:\", results[0])\n",
        "print(\"Test accuracy:\", results[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX0uFX93iAnF",
        "outputId": "92288775-3eac-42c5-eef1-3f0269d9c3cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "40/40 [==============================] - 4s 58ms/step - loss: 0.6905 - accuracy: 0.5299 - val_loss: 0.6843 - val_accuracy: 0.5846\n",
            "Epoch 2/10\n",
            "40/40 [==============================] - 1s 29ms/step - loss: 0.6620 - accuracy: 0.6683 - val_loss: 0.6302 - val_accuracy: 0.6966\n",
            "Epoch 3/10\n",
            "40/40 [==============================] - 1s 25ms/step - loss: 0.5742 - accuracy: 0.7484 - val_loss: 0.5206 - val_accuracy: 0.7750\n",
            "Epoch 4/10\n",
            "40/40 [==============================] - 1s 26ms/step - loss: 0.4639 - accuracy: 0.8161 - val_loss: 0.4365 - val_accuracy: 0.8200\n",
            "Epoch 5/10\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 0.3955 - accuracy: 0.8377 - val_loss: 0.3946 - val_accuracy: 0.8350\n",
            "Epoch 6/10\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.3570 - accuracy: 0.8525 - val_loss: 0.3746 - val_accuracy: 0.8414\n",
            "Epoch 7/10\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.3336 - accuracy: 0.8633 - val_loss: 0.3603 - val_accuracy: 0.8464\n",
            "Epoch 8/10\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.3173 - accuracy: 0.8704 - val_loss: 0.3550 - val_accuracy: 0.8462\n",
            "Epoch 9/10\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.3046 - accuracy: 0.8753 - val_loss: 0.3502 - val_accuracy: 0.8490\n",
            "Epoch 10/10\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 0.2953 - accuracy: 0.8798 - val_loss: 0.3457 - val_accuracy: 0.8508\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.3360 - accuracy: 0.8542\n",
            "Test loss: 0.3360079228878021\n",
            "Test accuracy: 0.8542400002479553\n"
          ]
        }
      ]
    }
  ]
}